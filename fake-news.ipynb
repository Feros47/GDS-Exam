{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of      Unnamed: 0     id                domain        type  \\\n",
      "0             0    141               awm.com  unreliable   \n",
      "1             1    256     beforeitsnews.com        fake   \n",
      "2             2    700           cnnnext.com  unreliable   \n",
      "3             3    768               awm.com  unreliable   \n",
      "4             4    791  bipartisanreport.com   clickbait   \n",
      "..          ...    ...                   ...         ...   \n",
      "245         245  39259     beforeitsnews.com        fake   \n",
      "246         246  39468     beforeitsnews.com        fake   \n",
      "247         247  39477       www.newsmax.com         NaN   \n",
      "248         248  39550       www.newsmax.com         NaN   \n",
      "249         249  39558       www.newsmax.com         NaN   \n",
      "\n",
      "                                                   url  \\\n",
      "0    http://awm.com/church-congregation-brings-gift...   \n",
      "1    http://beforeitsnews.com/awakening-start-here/...   \n",
      "2    http://www.cnnnext.com/video/18526/never-hike-...   \n",
      "3    http://awm.com/elusive-alien-of-the-sea-caught...   \n",
      "4    http://bipartisanreport.com/2018/01/21/trumps-...   \n",
      "..                                                 ...   \n",
      "245  http://beforeitsnews.com/economy/2017/12/priso...   \n",
      "246  http://beforeitsnews.com/diy/2017/11/4-useful-...   \n",
      "247  https://www.newsmax.com/politics/michael-hayde...   \n",
      "248  https://www.newsmax.com/newsfront/antonio-saba...   \n",
      "249  https://www.newsmax.com/newsfront/bill-clinton...   \n",
      "\n",
      "                                               content  \\\n",
      "0    Sometimes the power of Christmas will make you...   \n",
      "1    AWAKENING OF 12 STRANDS of DNA – “Reconnecting...   \n",
      "2    Never Hike Alone: A Friday the 13th Fan Film U...   \n",
      "3    When a rare shark was caught, scientists were ...   \n",
      "4    Donald Trump has the unnerving ability to abil...   \n",
      "..                                                 ...   \n",
      "245  Prison for Rahm, God’s Work And Many Others\\n\\...   \n",
      "246  4 Useful Items for Your Tiny Home\\n\\nHeadline:...   \n",
      "247  Former CIA Director Michael Hayden said Thursd...   \n",
      "248  Antonio Sabato Jr. says Hollywood's liberal el...   \n",
      "249  Former U.S. President Bill Clinton on Monday c...   \n",
      "\n",
      "                     scraped_at                 inserted_at  \\\n",
      "0    2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "1    2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "2    2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "3    2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "4    2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "..                          ...                         ...   \n",
      "245  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
      "246  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
      "247  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
      "248  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
      "249  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
      "\n",
      "                     updated_at  \\\n",
      "0    2018-02-02 01:19:41.756664   \n",
      "1    2018-02-02 01:19:41.756664   \n",
      "2    2018-02-02 01:19:41.756664   \n",
      "3    2018-02-02 01:19:41.756664   \n",
      "4    2018-02-02 01:19:41.756664   \n",
      "..                          ...   \n",
      "245  2018-02-02 01:19:41.756664   \n",
      "246  2018-02-02 01:19:41.756664   \n",
      "247  2018-02-02 01:19:41.756664   \n",
      "248  2018-02-02 01:19:41.756664   \n",
      "249  2018-02-02 01:19:41.756664   \n",
      "\n",
      "                                                 title          authors  \\\n",
      "0    Church Congregation Brings Gift to Waitresses ...      Ruth Harris   \n",
      "1    AWAKENING OF 12 STRANDS of DNA – “Reconnecting...     Zurich Times   \n",
      "2    Never Hike Alone - A Friday the 13th Fan Film ...              NaN   \n",
      "3    Elusive ‘Alien Of The Sea ‘ Caught By Scientis...  Alexander Smith   \n",
      "4    Trump’s Genius Poll Is Complete & The Results ...  Gloria Christie   \n",
      "..                                                 ...              ...   \n",
      "245        Prison for Rahm, God’s Work And Many Others              NaN   \n",
      "246                  4 Useful Items for Your Tiny Home        Dimitry K   \n",
      "247  Michael Hayden: We Should Be 'Frightened' by T...      Todd Beamon   \n",
      "248  Antonio Sabato Jr.: It's Oprah or Bust for Hol...    Bill Hoffmann   \n",
      "249  Bill Clinton Calls for Release of Reuters Jour...              NaN   \n",
      "\n",
      "     keywords                                      meta_keywords  \\\n",
      "0         NaN                                               ['']   \n",
      "1         NaN                                               ['']   \n",
      "2         NaN                                               ['']   \n",
      "3         NaN                                               ['']   \n",
      "4         NaN                                               ['']   \n",
      "..        ...                                                ...   \n",
      "245       NaN                                               ['']   \n",
      "246       NaN                                               ['']   \n",
      "247       NaN  ['michael hayden', 'sthole countries', 'daca',...   \n",
      "248       NaN  ['antonio sabato jr', 'oprah winfrey', 'presid...   \n",
      "249       NaN  ['bill clinton', 'myanmar', 'calls', 'release'...   \n",
      "\n",
      "                                      meta_description  \\\n",
      "0                                                  NaN   \n",
      "1                                                  NaN   \n",
      "2    Never Hike Alone: A Friday the 13th Fan Film  ...   \n",
      "3                                                  NaN   \n",
      "4                                                  NaN   \n",
      "..                                                 ...   \n",
      "245                                                NaN   \n",
      "246                                                NaN   \n",
      "247  President Donald Trump's reported remarks abou...   \n",
      "248  Antonio Sabato Jr. says Hollywood's liberal el...   \n",
      "249  Former U.S. President Bill Clinton Calls for R...   \n",
      "\n",
      "                                                  tags  summary  \n",
      "0                                                  NaN      NaN  \n",
      "1                                                  NaN      NaN  \n",
      "2                                                  NaN      NaN  \n",
      "3                                                  NaN      NaN  \n",
      "4                                                  NaN      NaN  \n",
      "..                                                 ...      ...  \n",
      "245                                                NaN      NaN  \n",
      "246                                                NaN      NaN  \n",
      "247  Homeland Security, Trump Administration, Immig...      NaN  \n",
      "248  Trump Administration, ISIS/Islamic State, News...      NaN  \n",
      "249  Donald Trump, Russia, Trump Administration, Gu...      NaN  \n",
      "\n",
      "[250 rows x 16 columns]>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv'\n",
    "df = pd.read_csv(url)\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in tokenized sample: [('<num>', 2558), ('one', 444), ('people', 386), ('trump', 347), ('like', 328), ('would', 323), ('time', 303), ('us', 284), ('also', 276), ('new', 270)]\n",
      "Top 10 words in original sample: [('the', 9379), ('of', 4943), ('to', 4841), ('and', 4643), ('a', 3317), ('in', 3064), ('is', 2262), ('that', 2109), ('for', 1528), ('are', 1210)]\n",
      "Top 10 words in stemmed sample: [('<num>', 2558), ('one', 468), ('like', 407), ('time', 405), ('peopl', 389), ('state', 370), ('trump', 350), ('would', 323), ('use', 323), ('market', 298)]\n",
      "Number of URLs: 209\n",
      "Number of dates: 0\n",
      "Number of numerics: 2558\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_word_frequency(counter, top_n=10000, title=\"Word Frequency Distribution\"):\n",
    "    \"\"\"\n",
    "    Plots the frequency distribution of the top_n words using a log-log plot.\n",
    "    \"\"\"\n",
    "    # Extract frequencies of the most common words\n",
    "    freqs = [freq for word, freq in counter.most_common(top_n)]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.loglog(range(1, len(freqs) + 1), freqs, marker=\".\")\n",
    "    plt.xlabel(\"Rank of word (log scale)\")\n",
    "    plt.ylabel(\"Frequency (log scale)\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Regex pattern for tokenization\n",
    "# <\\w+> matches tags (e.g., <num>), [\\w]+(?:-[\\w]+)? matches words with hyphens\n",
    "pattern = r\"<\\w+>|[\\w]+(?:-[\\w]+)?\"\n",
    "tokenizer = RegexpTokenizer(pattern)\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans a text string by replacing dates, emails, URLs, and numbers with placeholder tokens.\n",
    "    \"\"\"\n",
    "    # Replace dates (formats: YYYY-MM-DD and DD-MM-YYYY)\n",
    "    text = re.sub(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', '<DATE>', text)\n",
    "    text = re.sub(r'\\b\\d{2}-\\d{2}-\\d{4}\\b', '<DATE>', text)\n",
    "    # Replace email addresses\n",
    "    text = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '<EMAIL>', text)\n",
    "    # Replace URLs (http/https)\n",
    "    text = re.sub(r'https?://\\S+', '<URL>', text)\n",
    "    # Replace numbers (including decimals)\n",
    "    text = re.sub(r'\\b\\d+(\\.\\d+)?\\b', '<NUM>', text)\n",
    "    \n",
    "    return text.strip().lower()\n",
    "\n",
    "def tokenize_text(text: str, stop_words: set) -> list:\n",
    "    \"\"\"\n",
    "    Tokenizes the input text using NLTK and removes stopwords.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "def stem_tokens(tokens: list) -> list:\n",
    "    \"\"\"\n",
    "    Applies Porter stemming to a list of tokens.\n",
    "    \"\"\"\n",
    "    ps = PorterStemmer()\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "\n",
    "def main(df, text_column: str = 'content'):\n",
    "    # Ensure we work with the target text column only\n",
    "    df[text_column] = df[text_column].astype(str)\n",
    "    \n",
    "    # Define English stopwords set\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Clean and tokenize the specified text column\n",
    "    tokenized_series = df[text_column].apply(lambda x: \" \".join(tokenize_text(clean_text(x), stop_words)))\n",
    "    tokenized_series.to_csv(\"block_stopwords_news_sample.csv\", index=False)\n",
    "    \n",
    "    # Count word frequencies in the tokenized text\n",
    "    all_tokens = list(chain.from_iterable(tokenized_series.str.split()))\n",
    "    token_freq = Counter(all_tokens)\n",
    "    \n",
    "    # Count word frequencies in the original (lowercased) text for comparison\n",
    "    original_tokens = list(chain.from_iterable(df[text_column].str.lower().str.split()))\n",
    "    original_freq = Counter(original_tokens)\n",
    "    \n",
    "    print(f\"Top 10 words in tokenized sample: {token_freq.most_common(10)}\")\n",
    "    print(f\"Top 10 words in original sample: {original_freq.most_common(10)}\")\n",
    "    \n",
    "    # Apply stemming to the tokenized text\n",
    "    stemmed_series = tokenized_series.apply(lambda x: \" \".join(stem_tokens(x.split())))\n",
    "    stemmed_series.to_csv(\"stemmed_news_sample.csv\", index=False)\n",
    "    \n",
    "    # Count word frequencies in the stemmed text\n",
    "    all_stemmed_tokens = list(chain.from_iterable(stemmed_series.str.split()))\n",
    "    stem_freq = Counter(all_stemmed_tokens)\n",
    "    print(f\"Top 10 words in stemmed sample: {stem_freq.most_common(10)}\")\n",
    "    \n",
    "    # Use vectorized string methods to count the placeholder tokens\n",
    "    url_count = stemmed_series.str.count(\"<url>\").sum()\n",
    "    date_count = stemmed_series.str.count(\"<date>\").sum()\n",
    "    num_count = stemmed_series.str.count(\"<num>\").sum()\n",
    "    \n",
    "    print(f\"Number of URLs: {url_count}\")\n",
    "    print(f\"Number of dates: {date_count}\")\n",
    "    print(f\"Number of numerics: {num_count}\")\n",
    "    plot_word_frequency(original_freq, top_n=10000, title=\"Original Text Frequency Distribution\")\n",
    "    plot_word_frequency(token_freq, top_n=10000, title=\"Stopword-Removed Frequency Distribution\")\n",
    "    plot_word_frequency(stem_freq, top_n=10000, title=\"Stemmed Frequency Distribution\")\n",
    "\n",
    "\n",
    "# Read the CSV file into a DataFrame and select a subset (for testing)\n",
    "df = pd.read_csv(\"Datasets/news_sample.csv\", encoding=\"utf-8\")\n",
    "df_sample = df.head(300)  # For a large dataset, consider processing in chunks\n",
    "main(df_sample, text_column='content')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using Pandas and NLTK because Pandas offers a robust data handling and cleaning capabilities, which is essential for large datasets. While NLTK Provides a suite of NLP tools (tokenization, stopwords, stemming) that are standard in text preprocessing. These operations help normalize the data for downstream analysis and modeling.\n",
    "\n",
    "Pandas DataFrame: It’s flexible, easy to filter and analyze, and supports integration with visualization libraries.\n",
    "Rationale: A DataFrame structure is ideal for handling large datasets and performing exploratory data analysis (EDA).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying my data pipeline to the 995000 rows sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikla\\AppData\\Local\\Temp\\ipykernel_7620\\3583069079.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[text_column] = df[text_column].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in tokenized sample: [('<num>', 4162), ('said', 494), ('one', 364), ('new', 330), ('people', 326), ('would', 295), ('mr', 233), ('also', 227), ('trump', 222), ('state', 195)]\n",
      "Top 10 words in original sample: [('the', 8624), ('of', 4322), ('to', 4098), ('and', 3681), ('a', 3309), ('in', 2827), ('that', 1767), ('is', 1510), ('for', 1386), ('on', 1099)]\n",
      "Top 10 words in stemmed sample: [('<num>', 4162), ('said', 494), ('one', 387), ('state', 385), ('time', 332), ('peopl', 331), ('new', 330), ('would', 295), ('year', 267), ('like', 238)]\n",
      "Number of URLs: 60\n",
      "Number of dates: 2\n",
      "Number of numerics: 4162\n"
     ]
    }
   ],
   "source": [
    "subsetSample = pd.read_csv(\"Datasets\\\\995000_rows.csv\", encoding=\"utf-8\", low_memory=False)\n",
    "\n",
    "main(subsetSample.head(300), text_column='content')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns (0,1) have mixed types?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
