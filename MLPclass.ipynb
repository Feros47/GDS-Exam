{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLPclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (content sample):\n",
      "134    workplac distract consid part consumpt instead...\n",
      "129    ttip brexit moder terrorist 21st centuri wire ...\n",
      "146    european union act like aggress colonialist st...\n",
      "47     drain swamp doesnt matter long groupthink pers...\n",
      "59     justic depart ask suprem court overturn daca r...\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned dataset with selected columns\n",
    "df_clean = pd.read_csv('Datasets/news_sample_rows_cleaned.csv', usecols=['content', 'type', 'title'], dtype=str).dropna()\n",
    "\n",
    "# Map article types to binary labels (1: fake-related, 0: reliable)\n",
    "type_mapping = {'fake': 1, 'conspiracy': 1, 'junksci': 1, 'bias': 1, \n",
    "                'clickbait': 0, 'political': 0, 'reliable': 0}\n",
    "df_clean['label'] = df_clean['type'].map(type_mapping)\n",
    "df_clean = df_clean.dropna(subset=['label'])\n",
    "df_clean['label'] = df_clean['label'].astype(int)\n",
    "\n",
    "# Drop rows with missing content or title\n",
    "df_clean = df_clean.dropna(subset=['content', 'title'])\n",
    "\n",
    "# Split into training (80%), validation (10%), and test (10%) sets using stratification\n",
    "train_df, temp_df = train_test_split(df_clean, test_size=0.2, random_state=42, stratify=df_clean['label'])\n",
    "validation_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n",
    "\n",
    "# Extract features and labels for further use\n",
    "# content_train, title_train, y_train = train_df['content'], train_df['title'], train_df['label']\n",
    "# content_val, title_val, y_val = validation_df['content'], validation_df['title'], validation_df['label']\n",
    "# content_test, title_test, y_test = test_df['content'], test_df['title'], test_df['label']\n",
    "\n",
    "# -- Extract features and labels for further use --\n",
    "# -- concatenate content and title --\n",
    "combined_text, y_train = train_df['title'] + \" \" + train_df[\"content\"], train_df['label']\n",
    "combined_val, y_val = validation_df['title'] + \" \" + validation_df[\"content\"], validation_df['label']\n",
    "combined_test, y_test = test_df['title'] + \" \" + test_df[\"content\"], test_df['label']\n",
    "\n",
    "# -- Combine content and title into one --\n",
    "# train_df[\"combined_text\"] = train_df['title'] + \" \" + train_df['content']\n",
    "\n",
    "print(\"Training set (content sample):\")\n",
    "# print(content_train.head())\n",
    "print(combined_text.head())\n",
    "# print(\"\\nTraining set (title sample):\")\n",
    "# print(title_train.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(x_train, y_train, x_val, model_name):\n",
    "    start_time = time.time()\n",
    "    # Define a simple MLP with one hidden layer\n",
    "    mlp = MLPClassifier(max_iter=1000, random_state=42)\n",
    "    params = {\n",
    "        'hidden_layer_sizes': [(50,), (100,)],\n",
    "        'alpha': [1e-4, 1e-3, 1e-2]\n",
    "    }\n",
    "    # Use GridSearchCV for hypterparameter tuning\n",
    "    grid = GridSearchCV(\n",
    "        mlp, params, cv=3, n_jobs=-1, scoring='f1', pre_dispatch=3\n",
    "    )\n",
    "    grid.fit(x_train, y_train)\n",
    "    \n",
    "    print(f\"Neural Network training time: {(time.time() - start_time)/60:.2f} min\")\n",
    "    print(\"Best Parameters for Neural Network:\", grid.best_params_)\n",
    "    \n",
    "    # Save the best estimator\n",
    "    from joblib import dump\n",
    "    dump(grid, f'models/{model_name}.joblib')\n",
    "    \n",
    "    # Return predictions on x_val\n",
    "    return grid.predict(x_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'content_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_train, X_val\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# --- Evaluate advanced models using TF-IDF (1-gram) ---\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m X_train_tfidf, X_val_tfidf \u001b[38;5;241m=\u001b[39m make_TFIDF(\u001b[38;5;241m3500\u001b[39m, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[0;32mIn[20], line 13\u001b[0m, in \u001b[0;36mmake_TFIDF\u001b[0;34m(features, ngrams)\u001b[0m\n\u001b[1;32m      2\u001b[0m tfidf_pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[1;32m      3\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectorizer\u001b[39m\u001b[38;5;124m'\u001b[39m, TfidfVectorizer(lowercase\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m      4\u001b[0m                                    max_features\u001b[38;5;241m=\u001b[39mfeatures, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler(with_mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m     10\u001b[0m ])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Build TF-IDF for both content and title; then combine\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m content_train_tfidf \u001b[38;5;241m=\u001b[39m tfidf_pipeline\u001b[38;5;241m.\u001b[39mfit_transform(content_train, y_train)\n\u001b[1;32m     14\u001b[0m content_val_tfidf   \u001b[38;5;241m=\u001b[39m tfidf_pipeline\u001b[38;5;241m.\u001b[39mtransform(content_val)\n\u001b[1;32m     15\u001b[0m title_train_tfidf   \u001b[38;5;241m=\u001b[39m tfidf_pipeline\u001b[38;5;241m.\u001b[39mfit_transform(title_train, y_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'content_train' is not defined"
     ]
    }
   ],
   "source": [
    "def make_TFIDF(features, ngrams):\n",
    "    tfidf_pipeline = Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer(lowercase=False, \n",
    "                                       max_features=features, \n",
    "                                       min_df=1, \n",
    "                                       max_df=0.9, \n",
    "                                       token_pattern=r'<[\\w]+>|[\\w]+',\n",
    "                                       ngram_range=ngrams)),\n",
    "        ('scaler', StandardScaler(with_mean=False))\n",
    "    ])\n",
    "    \n",
    "    # Build TF-IDF for both content and title; then combine\n",
    "    content_train_tfidf = tfidf_pipeline.fit_transform(content_train, y_train)\n",
    "    content_val_tfidf   = tfidf_pipeline.transform(content_val)\n",
    "    title_train_tfidf   = tfidf_pipeline.fit_transform(title_train, y_train)\n",
    "    title_val_tfidf     = tfidf_pipeline.transform(title_val)\n",
    "    \n",
    "    X_train = hstack((content_train_tfidf, title_train_tfidf))\n",
    "    X_val   = hstack((content_val_tfidf, title_val_tfidf))\n",
    "    return X_train, X_val\n",
    "\n",
    "# --- Evaluate advanced models using TF-IDF (1-gram) ---\n",
    "X_train_tfidf, X_val_tfidf = make_TFIDF(3500, (1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF feature extraction (using existing code)\n",
    "X_train_tfidf, X_val_tfidf = make_TFIDF(3500, (1, 1))\n",
    "\n",
    "# Train & evaluate Neural Network (one hidden layer)\n",
    "print(\"\\nEvaluating Neural Network with TF-IDF (1-gram):\")\n",
    "y_pred_nn = train_neural_network(X_train_tfidf, y_train, X_val_tfidf, 'nn_1gram')\n",
    "print(\"Neural Network F1 score:\", metrics.f1_score(y_val, y_pred_nn))\n",
    "print(\"Neural Network Accuracy:\", metrics.accuracy_score(y_val, y_pred_nn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate the trained Neural Network on the test set ---\n",
    "from joblib import load\n",
    "\n",
    "# Load the NN model\n",
    "nn_model_loaded = load('models/nn_1gram.joblib')\n",
    "\n",
    "# Build TF-IDF for the test set\n",
    "test_tfidf_pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(\n",
    "        lowercase=False, \n",
    "        max_features=3500, \n",
    "        min_df=1, \n",
    "        max_df=0.9, \n",
    "        token_pattern=r'<[\\w]+>|[\\w]+',\n",
    "        ngram_range=(1, 1))\n",
    "    ),\n",
    "    ('scaler', StandardScaler(with_mean=False))\n",
    "])\n",
    "\n",
    "# Convert test content and title to TF-IDF and combine\n",
    "content_test_tfidf = test_tfidf_pipeline.fit_transform(content_test)\n",
    "title_test_tfidf   = test_tfidf_pipeline.fit_transform(title_test)\n",
    "X_test_tfidf       = hstack((content_test_tfidf, title_test_tfidf))\n",
    "\n",
    "# Predict and evaluate\n",
    "nn_pred_test = nn_model_loaded.predict(X_test_tfidf)\n",
    "print(\"\\nNeural Network on Test Set:\")\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, nn_pred_test))\n",
    "print(\"F1 score:\", metrics.f1_score(y_test, nn_pred_test))\n",
    "make_confusion_matrix(y_test, nn_pred_test, \"Neural Network Model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important note\n",
    "Vectorizer learns which features to use from the document.\n",
    "Therefore the learned features from the training data should\n",
    "be the same one used on the test data. If we learn new features\n",
    "from the test data we will have a mismatch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
